{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from mixture.ipynb\n",
      "pi[0] [0.085]\n",
      "pi[1] [0.13]\n",
      "theta[0, 239] 0.6427106227106232\n",
      "theta[3, 298] 0.46573612495845823\n",
      "Training log-likelihood: -172.07854196938325\n",
      "Test log-likelihood: -170.97538133747267\n",
      "R[0, 2] 0.1748895149211724\n",
      "R[1, 0] 0.688537676109229\n",
      "P[0, 183] 0.6516151998131035\n",
      "P[2, 628] 0.4740801724913304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\eodus\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\matplotlib\\axes\\_base.py:3012: UserWarning: Attempting to set identical left==right results\n",
      "in singular transformations; automatically expanding.\n",
      "left=1.0, right=1.0\n",
      "  self.set_xlim(upper, lower, auto=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training log-likelihood: -138.06314474092903\n",
      "Final test log-likelihood: -138.54618064790586\n",
      "Train set case:  (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949],\n",
      "      dtype=int64))\n",
      "Test set case:  (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([ 980, 1135, 1032, 1010,  982,  892,  958, 1028,  974, 1009],\n",
      "      dtype=int64))\n",
      "Training set\n",
      "Average log-probability of a 0 image: -201.136\n",
      "Average log-probability of a 1 image: -97.755\n",
      "Average log-probability of a 2 image: -201.701\n",
      "Average log-probability of a 3 image: -184.880\n",
      "Average log-probability of a 4 image: -172.018\n",
      "Average log-probability of a 5 image: -191.928\n",
      "Average log-probability of a 6 image: -177.253\n",
      "Average log-probability of a 7 image: -156.895\n",
      "Average log-probability of a 8 image: -187.626\n",
      "Average log-probability of a 9 image: -162.041\n",
      "\n",
      "Test set\n",
      "Average log-probability of a 0 image: -199.743\n",
      "Average log-probability of a 1 image: -96.973\n",
      "Average log-probability of a 2 image: -199.290\n",
      "Average log-probability of a 3 image: -182.329\n",
      "Average log-probability of a 4 image: -171.179\n",
      "Average log-probability of a 5 image: -190.813\n",
      "Average log-probability of a 6 image: -181.537\n",
      "Average log-probability of a 7 image: -154.226\n",
      "Average log-probability of a 8 image: -187.226\n",
      "Average log-probability of a 9 image: -159.568\n",
      "\n",
      "Train set case:  (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949],\n",
      "      dtype=int64))\n",
      "Test set case:  (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([ 980, 1135, 1032, 1010,  982,  892,  958, 1028,  974, 1009],\n",
      "      dtype=int64))\n",
      "Training set\n",
      "Average log-probability of a 0 image: -158.214\n",
      "Average log-probability of a 1 image: -68.176\n",
      "Average log-probability of a 2 image: -168.260\n",
      "Average log-probability of a 3 image: -154.802\n",
      "Average log-probability of a 4 image: -139.595\n",
      "Average log-probability of a 5 image: -152.925\n",
      "Average log-probability of a 6 image: -142.516\n",
      "Average log-probability of a 7 image: -122.211\n",
      "Average log-probability of a 8 image: -158.338\n",
      "Average log-probability of a 9 image: -126.987\n",
      "\n",
      "Test set\n",
      "Average log-probability of a 0 image: -159.206\n",
      "Average log-probability of a 1 image: -67.884\n",
      "Average log-probability of a 2 image: -167.031\n",
      "Average log-probability of a 3 image: -153.816\n",
      "Average log-probability of a 4 image: -139.915\n",
      "Average log-probability of a 5 image: -153.876\n",
      "Average log-probability of a 6 image: -146.322\n",
      "Average log-probability of a 7 image: -121.600\n",
      "Average log-probability of a 8 image: -161.038\n",
      "Average log-probability of a 9 image: -126.834\n",
      "\n",
      "The E-step seems OK.\n",
      "The theta update seems OK.\n",
      "The pi update seems OK.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#import mixture\n",
    "import import_ipynb\n",
    "import mixture\n",
    "import util\n",
    "\n",
    "\n",
    "\n",
    "def multinomial_entropy(p):\n",
    "    \"\"\"Compute the entropy of a Bernoulli random variable, in nats rather than bits.\"\"\"\n",
    "    p = np.clip(p, 1e-20, np.infty)      # avoid taking the log of 0\n",
    "    return -np.sum(p * np.log(p))\n",
    "\n",
    "\n",
    "def variational_objective(model, X, R, pi, theta):\n",
    "    \"\"\"Compute the variational lower bound on the log-likelihood that each step of E-M\n",
    "    is maximizing. This is described in the paper\n",
    "\n",
    "        Neal and Hinton, 1998. A view of the E-M algorithm that justifies incremental, sparse, and other variants.\n",
    "\n",
    "    We can test the update rules by verifying that each step maximizes this bound.\n",
    "    \"\"\"\n",
    "\n",
    "    model = mixture.Model(model.prior, mixture.Params(pi, theta))\n",
    "    expected_log_prob = model.expected_joint_log_probability(X, R)\n",
    "    entropy_term = np.sum(multinomial_entropy(R))\n",
    "    return expected_log_prob + entropy_term\n",
    "\n",
    "def perturb_pi(pi, eps=1e-6):\n",
    "    pi = np.random.normal(pi, eps)\n",
    "    pi = np.clip(pi, 1e-10, np.infty)\n",
    "    pi /= pi.sum()\n",
    "    return pi\n",
    "\n",
    "def perturb_theta(theta, eps=1e-6):\n",
    "    theta = np.random.normal(theta, eps)\n",
    "    theta = np.clip(theta, 1e-10, 1. - 1e-10)\n",
    "    return theta\n",
    "\n",
    "def perturb_R(R, eps=1e-6):\n",
    "    R = np.random.normal(R, eps)\n",
    "    R = np.clip(R, 1e-10, np.infty)\n",
    "    R /= R.sum(1).reshape((-1, 1))\n",
    "    return R\n",
    "\n",
    "\n",
    "\n",
    "def check_m_step():\n",
    "    \"\"\"Check that the M-step updates by making sure they maximize the variational\n",
    "    objective with respect to the model parameters.\"\"\"\n",
    "    np.random.seed(0)\n",
    "\n",
    "    NUM_IMAGES = 100\n",
    "\n",
    "    X = util.read_mnist_images(mixture.TRAIN_IMAGES_FILE)\n",
    "    X = X[:NUM_IMAGES, :]\n",
    "    R = np.random.uniform(size=(NUM_IMAGES, 10))\n",
    "    R /= R.sum(1).reshape((-1, 1))\n",
    "\n",
    "    model = mixture.Model.random_initialization(mixture.Prior.default_prior(), 10, 784)\n",
    "    \n",
    "    theta = model.update_theta(X, R)\n",
    "    pi = model.update_pi(R)\n",
    "    \n",
    "\n",
    "    opt = variational_objective(model, X, R, pi, theta)\n",
    "    \n",
    "    ok = True\n",
    "    for i in range(20):\n",
    "        new_theta = perturb_theta(theta)\n",
    "        new_obj = variational_objective(model, X, R, pi, new_theta)\n",
    "        if new_obj > opt:\n",
    "            ok = False\n",
    "    if ok:\n",
    "        print('The theta update seems OK.')\n",
    "    else:\n",
    "        print('Something seems to be wrong with the theta update.')\n",
    "\n",
    "    if not np.allclose(np.sum(pi), 1.):\n",
    "        print('Uh-oh. pi does not seem to sum to 1.')\n",
    "    else:\n",
    "        ok = True\n",
    "        for i in range(20):\n",
    "            new_pi = perturb_pi(pi)\n",
    "            new_obj = variational_objective(model, X, R, new_pi, theta)\n",
    "            if new_obj > opt:\n",
    "                ok = False\n",
    "        if ok:\n",
    "            print('The pi update seems OK.')\n",
    "        else:\n",
    "            print('Something seems to be wrong with the pi update.')\n",
    "\n",
    "\n",
    "def check_e_step():\n",
    "    \"\"\"Check the E-step updates by making sure they maximize the variational\n",
    "    objective with respect to the responsibilities. Note that this does not\n",
    "    fully check your solution to Part 2, since it only applies to fully observed\n",
    "    images.\"\"\"\n",
    "    \n",
    "    np.random.seed(0)\n",
    "\n",
    "    NUM_IMAGES = 100\n",
    "\n",
    "    X = util.read_mnist_images(mixture.TRAIN_IMAGES_FILE)\n",
    "    X = X[:NUM_IMAGES, :]\n",
    "    model = mixture.train_from_labels(show=False)\n",
    "\n",
    "    # reduce the number of observations so that the posterior is less peaked\n",
    "    X = X[:, ::50]\n",
    "    model.params.theta = model.params.theta[:, ::50]\n",
    "    \n",
    "    R = model.compute_posterior(X)\n",
    "\n",
    "    opt = variational_objective(model, X, R, model.params.pi, model.params.theta)\n",
    "\n",
    "    if not np.allclose(R.sum(1), 1.):\n",
    "        print('Uh-oh. Rows of R do not seem to sum to 1.')\n",
    "    else:\n",
    "        ok = True\n",
    "        for i in range(20):\n",
    "            new_R = perturb_R(R)\n",
    "            new_obj = variational_objective(model, X, new_R, model.params.pi, model.params.theta)\n",
    "            if new_obj > opt:\n",
    "                ok = False\n",
    "        if ok:\n",
    "            print('The E-step seems OK.')\n",
    "        else:\n",
    "            print('Something seems to be wrong with the E-step.')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    check_e_step()\n",
    "    check_m_step()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
